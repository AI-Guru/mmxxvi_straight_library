# CLAUDE.md — Straight Library

## Project Overview

Digital library browser that exposes book metadata, summaries, and full texts via three interfaces: REST API (FastAPI, port 9821), MCP Server (FastMCP, port 9823), and a Web UI. All data stored in PostgreSQL 17 with pgvector for semantic search and native FTS for keyword search. Embeddings generated by Ollama (`qwen3-embedding:0.6b`, 1024 dimensions).

## Architecture

```
┌─────────────────┐     ┌──────────────┐     ┌──────────────────┐
│   Web UI (SPA)  │────▶│  FastAPI      │────▶│  PostgreSQL 17   │
│   /frontend/    │     │  /api/        │     │  + pgvector      │
└─────────────────┘     └──────┬───────┘     └──────────────────┘
                               │                      ▲
┌─────────────────┐            │              ┌───────┴──────┐
│  FastMCP Server │────────────┘              │  Ollama      │
│  /mcp/          │                           │  (embeddings)│
└─────────────────┘                           └──────────────┘
```

- **api/** — FastAPI service: upload, browse, paginate, search (FTS + semantic)
- **mcp/** — FastMCP server: 6 tools for AI agent integration
- **frontend/** — Vanilla JS/HTML/CSS cyberpunk dark theme UI (served at `/static`)
- **init/** — PostgreSQL init SQL
- **prepare_library.py** — CLI: combines metadata + markdown files into `_libraryentry.md`
- **upload_library.py** — CLI: batch uploads `_libraryentry.md` files to the API

## Tech Stack

- **Python 3.11+**, FastAPI, Uvicorn, Pydantic
- **PostgreSQL 17** with pgvector extension
- **LangGraph** AsyncPostgresStore for vector embeddings
- **LangChain + Ollama** for embedding generation
- **FastMCP** for MCP server
- **Docker + Docker Compose** for deployment
- **UV** package manager in containers

## Quick Start

```bash
# 1. Prepare library entries from source markdown/JSON
python prepare_library.py

# 2. Start services (requires Ollama running with qwen3-embedding:0.6b)
docker compose up --build -d

# 3. Batch upload entries
python upload_library.py --api-url http://localhost:9821 --data-dir data --workers 4

# 4. Open http://localhost:9821
```

## Key Conventions

- **Section names** are lowercase: `shortsummary`, `summary`, `fulltext`
- **Entry IDs** are 16-char lowercase hex SHA256 hashes
- **Page numbers** are 1-based; 0 pages = empty section
- **Pagination** targets ~4000 chars per page (~1000 tokens), split by paragraph blocks
- **Chapters** extracted from markdown headings at upload time
- **Async throughout** — all DB operations use async/await with connection pooling
- **Non-fatal embedding errors** — if Ollama is down, DB data still commits

## Ports

| Service    | Port |
|------------|------|
| API        | 9821 |
| PostgreSQL | 9822 |
| MCP        | 9823 |

## Configuration

Environment variables via `.env` (see `.env.example`). Key settings:
- `PAGE_MAX_CHARS` (default 4000), `CHUNK_SIZE` (1000), `CHUNK_OVERLAP` (100)
- `OLLAMA_HOST`, `OLLAMA_EMBED_MODEL`, `EMBEDDING_DIMENSION`
- `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`

## Database

Six core tables: `metadata`, `shortsummary`, `summary`, `fulltext`, `chapters`, `content_fts`. Foreign keys cascade on delete. FTS uses `tsvector` with GIN indexes and `websearch_to_tsquery`. pgvector tables are auto-created by LangGraph's AsyncPostgresStore.

## File Format

`_libraryentry.md` files use YAML frontmatter + 3 content sections separated by `---`:

```
---
title: ...
author: ...
publication_year: ...
genre: ...
custom_tags: [...]
---
shortsummary content
---
summary content
---
fulltext content
```

## Data

The `data/` directory is gitignored. Book source files live in `data/books-collection/MyCollection/` organized by letter subdirectories.

## Notes

- No authentication — read-only access by design
- CORS is open (all origins)
- The Web UI is vanilla JS with no build step
- MCP server uses streamable-http transport
